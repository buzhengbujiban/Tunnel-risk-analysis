# æœ¬æ¬¡é¡¹ç›®è®¾è®¡ä»£ç ä¸»è¦åŒ…å«äº”å¤§æ¨¡å—ï¼š
## *ç›®æ ‡æ£€æµ‹*ã€*è¯­ä¹‰åˆ†å‰²ä¸èšç±»*ã€*ç®€å•åˆ†ç±»*ã€*å¤§æ¨¡å‹åˆ†ç±»*ã€*ç²¾ç¡®åº¦åˆ†æ*


# ä»£ç æ–‡ä»¶ç»“æ„ï¼š
>**code**
>>yolov5-master  **ç›®æ ‡æ£€æµ‹**

>>forVGG   **å¤§æ¨¡å‹åˆ†ç±»**

>>codall   **è¯­ä¹‰åˆ†å‰²ä¸èšç±»**

>>datasets   **æ•°æ®é›†ï¼ˆéƒ¨åˆ†ç¤ºä¾‹ï¼‰**

>>inpainting **åˆæˆæ•°æ®æŠ å›¾**

>>GracoNet-Object-Placement **åˆæˆæ•°æ®æ”¾ç½®**

>>SVMImageClassification-master **ç®€å•åˆ†ç±»æ¨¡å‹**

>>display **å…¶ä»–æ–‡ä»¶**


# 0.System requirement 

æˆ‘ä»¬å»ºè®®ä½¿ç”¨ `linux Ubuntu` version>=Ubuntu 21.10  å®Œæˆæœ¬æ¬¡ä»£ç çš„è¿è¡Œ

use  `cat /usr/local/cuda/version.txt` to see cuda version.  GPU is recomended and Cuda>=10.2


```python
from PIL import Image
```

# 0.datasets

æ•°æ®é›†å’Œå®Œæ•´æ€§ä»£ç æ”¾ç½®åœ¨æœåŠ¡å™¨ç«¯ï¼ŒæŸ¥çœ‹æˆ‘ä»¬çš„æ•°æ®é›†å¯ä»¥ä½¿ç”¨`ssh`äºæˆ‘ä»¬çš„æœåŠ¡å™¨å»ºç«‹è¿æ¥ã€‚




```python
!ssh user1@5.tcp.vip.cpolar.cn -p 10345  # port is (dynamic) changing with in 24 h. 
```

    ^C
    

æˆ‘ä»¬ä½¿ç”¨äº†cplaræ„å»ºå†…ç½‘ç©¿é€ï¼Œå¹¶å»ºç«‹äº†ç¨³å®šçš„TCPï¼Œç«¯å£å·ä¸åŒï¼Œç™»å½•æœåŠ¡å™¨è¯·å‘`auther` ç´¢è¦ç«¯å£å·å’Œå¯†ç 
è¯¦ç»†ä¿¡æ¯å‚è§[cpolarä½¿ç”¨](https://www.cpolar.com/) ã€[SSHè¿œç¨‹è¿æ¥linux](https://blog.csdn.net/Hellowenpan/article/details/82904109)

åˆå§‹æ•°æ®é›†ä½äº  `/home/user1/yolo/jiafei_projects/yolo_dataset` labels åŒ…å« `xml`å’Œ`txt` ä¸¤ç§

å›¾ç‰‡åˆ†ç±»æ¨¡å‹æ•°æ®é›†ä½äº `/home/user1/yolo/jiafei_projects/yolo_model/predict_res`

åŒæ—¶ä¹Ÿå¯ä»¥åœ¨ç»ˆç«¯è¾“å…¥å¦ä¸€æœåŠ¡å™¨sshè¿æ¥
å¦‚æœæƒ³è¦è¿œç¨‹è§‚å¯Ÿæˆ‘ä»¬çš„tensorboardç»“æœï¼Œè¯·æå‰å»ºç«‹ä»£ç†è¿æ¥å°†æœåŠ¡å™¨ç«¯`127.0.0.1:8080` ${tesorboard-logfile} ä¸æœ¬åœ°  `127.0.0.1:8877` å»ºç«‹è¿æ¥


```python
!ssh lws@47.97.51.98 -p 6001 -L 127.0.0.1:8887:127.0.0.1:8080
```


```python
display(Image.open('code/display/x_3_508_00.png'))
```


    
![png](tutorial_files/tutorial_10_0.png)
    



```python
display(Image.open('code/yolov5-master/runs/train/exp24/val_batch1_pred.jpg'))
```


    
![png](tutorial_files/tutorial_11_0.png)
    


# 1.ç›®æ ‡æ£€æµ‹

é‡‡ç”¨yolovæ¡†æ¶è¿›è¡Œç—…å®³æå–ï¼ŒåŸç†è¯¦è§ [COCO val 2017](https://github.com/ultralytics/yolov5/blob/74b34872fdf41941cddcf243951cdb090fbac17b/data/coco.yaml#L14) dataset (1GB - 5000 images)  
https://github.com/Hou-yanbin/yolov5-master

Inference  run  `python detect.py`ã€ train with our own datasets  run `python train.py --{your expir name}`

é‡‡å–`block` \ `leak` \ `crack` ä¸‰ç§ç—…å®³æ ‡æ³¨å¹¶æå–



```python
# Run YOLOv5x on COCO val
!python val.py --weights yolov5x.pt --data coco.yaml --img 640 --iou 0.65 --half
```

    [34m[1mval: [0mdata=/content/yolov5/data/coco.yaml, weights=['yolov5x.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.65, task=val, device=, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=True, project=runs/val, name=exp, exist_ok=False, half=True
    YOLOv5 ğŸš€ v6.0-48-g84a8099 torch 1.10.0+cu102 CUDA:0 (Tesla V100-SXM2-16GB, 16160MiB)
    
    Downloading https://github.com/ultralytics/yolov5/releases/download/v6.0/yolov5x.pt to yolov5x.pt...
    100% 166M/166M [00:03<00:00, 54.1MB/s]
    
    Fusing layers... 
    Model Summary: 444 layers, 86705005 parameters, 0 gradients
    [34m[1mval: [0mScanning '../datasets/coco/val2017' images and labels...4952 found, 48 missing, 0 empty, 0 corrupted: 100% 5000/5000 [00:01<00:00, 2636.64it/s]
    [34m[1mval: [0mNew cache created: ../datasets/coco/val2017.cache
                   Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 157/157 [01:12<00:00,  2.17it/s]
                     all       5000      36335      0.729       0.63      0.683      0.496
    Speed: 0.1ms pre-process, 4.9ms inference, 1.9ms NMS per image at shape (32, 3, 640, 640)
    
    Evaluating pycocotools mAP... saving runs/val/exp/yolov5x_predictions.json...
    loading annotations into memory...
    Done (t=0.46s)
    creating index...
    index created!
    Loading and preparing results...
    DONE (t=5.15s)
    creating index...
    index created!
    Running per image evaluation...
    Evaluate annotation type *bbox*
    DONE (t=90.39s).
    Accumulating evaluation results...
    DONE (t=14.54s).
     Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.507
     Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.689
     Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.552
     Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.345
     Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.559
     Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.652
     Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.381
     Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.630
     Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.682
     Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.526
     Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.732
     Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.829
    Results saved to [1mruns/val/exp[0m
    


```python
# Run YOLOv5x on COCO val
!python val.py --weights yolov5x.pt --data coco.yaml --img 640 --iou 0.65 --half
```

    [34m[1mval: [0mdata=/content/yolov5/data/coco.yaml, weights=['yolov5x.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.65, task=val, device=, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=True, project=runs/val, name=exp, exist_ok=False, half=True
    YOLOv5 ğŸš€ v6.0-48-g84a8099 torch 1.10.0+cu102 CUDA:0 (Tesla V100-SXM2-16GB, 16160MiB)
    
    Downloading https://github.com/ultralytics/yolov5/releases/download/v6.0/yolov5x.pt to yolov5x.pt...
    100% 166M/166M [00:03<00:00, 54.1MB/s]
    
    Fusing layers... 
    Model Summary: 444 layers, 86705005 parameters, 0 gradients
    [34m[1mval: [0mScanning '../datasets/coco/val2017' images and labels...4952 found, 48 missing, 0 empty, 0 corrupted: 100% 5000/5000 [00:01<00:00, 2636.64it/s]
    [34m[1mval: [0mNew cache created: ../datasets/coco/val2017.cache
                   Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 157/157 [01:12<00:00,  2.17it/s]
                     all       5000      36335      0.729       0.63      0.683      0.496
    Speed: 0.1ms pre-process, 4.9ms inference, 1.9ms NMS per image at shape (32, 3, 640, 640)
    
    Evaluating pycocotools mAP... saving runs/val/exp/yolov5x_predictions.json...
    loading annotations into memory...
    Done (t=0.46s)
    creating index...
    index created!
    Loading and preparing results...
    DONE (t=5.15s)
    creating index...
    index created!
    Running per image evaluation...
    Evaluate annotation type *bbox*
    DONE (t=90.39s).
    Accumulating evaluation results...
    DONE (t=14.54s).
     Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.507
     Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.689
     Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.552
     Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.345
     Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.559
     Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.652
     Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.381
     Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.630
     Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.682
     Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.526
     Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.732
     Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.829
    Results saved to [1mruns/val/exp[0m
    


```python
display(Image.open('code/display/display_yolov.png'))
```


    
![png](tutorial_files/tutorial_15_0.png)
    


see results by ssh `/home/user1/yolo/jiafei_projects/yolo_model/predict_res`


```python

display(Image.open('code/display/s_2_735_31.png'))
```


    
![png](tutorial_files/tutorial_17_0.png)
    


# 2. èšç±»

ä»£ç å¤ç°è¯¦ç»†å‚è§ `code\cluster_maskrcnn [A]clustering.ipynb [A]maskrcnnè¯„ä»·èšç±»ç»“æœ.ipynb` é‡Œé¢æœ‰è¯¦ç»†çš„æŒ‡å¯¼æ­¥éª¤

## + å»å™ª


```python
display(Image.open('code/cluster_maskrcnn/0001174.jpg'))
```


    
![png](tutorial_files/tutorial_20_0.png)
    


## + è…èš€è†¨èƒ€


```python

display(Image.open('code/cluster_maskrcnn/imggaus.png'))
```


    
![png](tutorial_files/tutorial_22_0.png)
    



```python

display(Image.open('code/cluster_maskrcnn/img11.png'))
```


    
![png](tutorial_files/tutorial_23_0.png)
    



```python

display(Image.open('code/display/waji_denoise_k16.jpg'))

```


    
![png](tutorial_files/tutorial_24_0.png)
    


## use tensorboard 
run below 


```python
# Tensorboard  (optional)
%load_ext tensorboard
%tensorboard --logdir runs/train
```

# 3. maskrcnn

## - environment

>numpy;
>scipy;
>Pillow;
>cython;
;
>matplotlib;

>scikit-image;
>tensorflow>=1.3.0;
>keras>=2.0.8;
>opencv-python;

>h5py;
>imgaug;
>IPython[all]


```python

display(Image.open('code/display/forthat_20_mask_rcnn_out_py.jpg'))

```


    
![png](tutorial_files/tutorial_29_0.png)
    


# 4.åˆæˆæ•°æ®

inpainting+ objectPlacement
æˆ‘ä»¬å‚ç…§JiahuiYu å¯¹Inpaintingçš„åˆ†æè¿‡ç¨‹


@article{yu2018generative,
  title={Generative Image Inpainting with Contextual Attention},
  author={Yu, Jiahui and Lin, Zhe and Yang, Jimei and Shen, Xiaohui and Lu, Xin and Huang, Thomas S},
  journal={arXiv preprint arXiv:1801.07892},
  year={2018}
}

@article{yu2018free,
  title={Free-Form Image Inpainting with Gated Convolution},
  author={Yu, Jiahui and Lin, Zhe and Yang, Jimei and Shen, Xiaohui and Lu, Xin and Huang, Thomas S},
  journal={arXiv preprint arXiv:1806.03589},
  year={2018}
}

## Requirements:
Install python3  
Install tensorflow (tested on Release 1.3.0, 1.4.0, 1.5.0, 1.6.0, 1.7.0).  
Install tensorflow toolkit neuralgym (run pip install git+https://github.com/JiahuiYu/neuralgym).  

#### Training:  
Prepare training images filelist and shuffle it (example).  
Modify inpaint.yml to set DATA_FLIST, LOG_DIR, IMG_SHAPES and other parameters.  
Run python train.py.  

#### Resume training:
Modify `MODEL_RESTORE` flag in `inpaint.yml`. E.g., MODEL_RESTORE: 20180115220926508503_places2_model.  
Run `python train.py.`  
Testing:  
Run python test.py `--image examples/input.png --mask examples/mask.png --output examples/output.png --checkpoint`   `model_logs/your_model_dir.`  



```python
display(Image.open('code/inpainting/inpainting.png'))
```


    
![png](tutorial_files/tutorial_33_0.png)
    


see `code/inpainting/examples/places2` for the trial picture

see more result by `ssh lws@47.97.51.98 -p 6001`  \ `/home/lws/training_data/long`

### objectplacement 
### æˆ‘ä»¬å®ç°äº†

2023-05-01 22:27:59.832766
 - Accuracy = 0.600





### eval è¯¦è§ ssh
 `/home/user1/GracoNet-Object-Placement/MOCS2OPA`


```python
display(Image.open('code/display/granocate.png'))
```


    
![png](tutorial_files/tutorial_38_0.png)
    


# 5. åˆ†ç±»æ¨¡å‹
#### environment see `/code/SVMImageClassification-master/requirements.txt`
#### dependency see  `/code/SVMImageClassification-master/readme.txt`

one trial below

æˆ‘ä»¬çš„æ•°æ®é›†æ²¡æœ‰æ”¾å…¥æ–‡ä»¶ä¸­ï¼Œå¦‚æœè¦ç”¨éš§é“æ•°æ®é›†è¿è¡Œï¼Œè¯·ä¿®æ”¹å„ä¸ªæœºå™¨å­¦ä¹ ä»£ç ä¸­çš„ `os.listdir("photo2/%s" % i):` ä¸­çš„è·¯å¾„


```python
display(Image.open('code/SVMImageClassification-master/matrix.png'))

```


    
![png](tutorial_files/tutorial_40_0.png)
    


`code/forVGG/aevall.py` ä¸“é—¨ç”¨æ¥è¾“å‡ºAP precision_score æ··æ·†çŸ©é˜µç­‰å€¼


```python
array=([[2, 0, 0],
       [0, 0, 1],
       [1, 0, 2]])
array
```




    [[2, 0, 0], [0, 0, 1], [1, 0, 2]]



### æˆ‘ä»¬æ±‡æ€»å¾—åˆ°çš„å›¾åƒç‰¹å¾æå–æ–‡ä»¶ä¸º `train_data.csv`

# 6 åˆ†ç±»å¤§æ¨¡å‹

VGGæ¨¡å‹è®­ç»ƒ   
` cd code/forVGG/`  run ` python  agptesst.py` for inference     
 run  `train.py` for train
 
æ¨¡å‹å¾®è°ƒ  
è¿è¡Œ `python forVGG\Fintune_table\fintt.py` å°†è·¯å¾„è°ƒæ•´æˆæ•°æ®é›†æ‰€åœ¨ä½ç½®ï¼Œè¿è¡Œå¯è¿›è¡Œæ¨¡å‹å¾®è°ƒè®­ç»ƒ

## dependencyï¼š environment




```python

import torch
import torch.nn as nn
from torchvision import transforms, datasets
from torch.utils.data.dataloader import DataLoader
from torch import optim
from tqdm import tqdm
from torch.autograd import Variable



```


```python
display(Image.open('code/display/vgg.png'))
!python train.py
```


    
![png](tutorial_files/tutorial_47_0.png)
    



```python
display(Image.open('code/display/aptext.png'))
```


    
![png](tutorial_files/tutorial_48_0.png)
    



```python

```

# å¦å¤–æˆ‘ä»¬è¿˜ä½¿ç”¨DeepLearningExamples ä¸ºæˆ‘ä»¬å¤§æ¨¡å‹åˆ†ç±»åŠ ä»¥è®­ç»ƒ

### ç”¨ `DeepLearningExamples/PyTorch/Classification/GPUNet` é…ç½®
 train run `python train.py`  test run `python validate.py`  eval run `python eval.py`

# åŒæ—¶ä½¿ç”¨API è¿›è¡Œè¾…åŠ© ä¹Ÿæ˜¯é‡è¦æ–¹æ³•

### è¯¦ç»†å¯è§ [clrifai](https://clarifai.com/datastrategy/MAIN-image-moderation/models/moderation-all-resnext-2)
æˆ‘ä»¬ä½¿ç”¨ `withA.py`\ `withAPI.py` \ `withAPIforclassification.py`

æˆ‘ä»¬çš„workflow å¦‚ä¸‹


```python
display(Image.open('code/forVGG/our_workflow.png'))
```


    
![png](tutorial_files/tutorial_52_0.png)
    

